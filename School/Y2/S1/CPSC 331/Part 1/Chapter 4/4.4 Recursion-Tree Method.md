In a recursion tree a single node represents the cost of a single subproblem somewhere in the set of recursive function invocations. Usually you find the cost per level and then total all the levels to find the total cost.

Recursion tree method is best used to generate intuition for a good guess, but if you draw a precise recursion tree and sum the costs precisely it can be used for a proof itself. However using it for a guess allows you to simplify the math a lot.

# Illustrative Example
Recurrence: T(n) = 3T(n/4) + ϴ($n^2$).

![[Pasted image 20240922200747.png]]

Because subproblem sizes decrease by a factor of 4 every time we go down one level, we will eventually bottom out at a base case where n < n0. By convention, the base case is T(n) = ϴ(1) for n < n0, where n0 > 0 is a threshold constant past which the recurrence is well defined. Let's simplify the math a little.

Let's assume that n is an exact power of 4 and that the base case is T(1) = ϴ(1). It turns out these won't affect the solution.

What's the height of the tree? As we descend the tree from the root, the subproblem size hits n = 1 when n/$4^i$ = 1, aka when i = $log_4n$ .

The number of nodes at depth i is $3^i$, since the number of nodes increases by a factor of 3 each level. Since each subproblem's size decreases by a factor of 4 per level, the cost per problem at a depth i is c(n/$4^{i - 1})^2$ = $cn^216^i$$. Since there are $3^i$ nodes per level, the per level cost becomes $(3/16)^icn^2$. The bottom level, at depth $log_4n$, contains $3^{log_4n} = n^{log_43}$ leaves. Each leaf has a cost of ϴ(1), giving a cost of ϴ($n^{log_43}$) for that level.

Now we add up all the levels:
![[Pasted image 20240922202023.png]]
Line 4 is a formula about geometric sequences.

This gives T(n) = O(n^2). In fact, if O(n^2) is indeed an upper bound for the recurrence then it must also be a tight bound. Why? The first recursive call contributes a cost of theta(n^2), and so omega(n^2) must be a lower bound for the recurrence.

For the base case, we want to show that T(n) = theta(1) for n < n0, where n0 is a sufficiently large threshold constant past which the recurrence is well defined. To do this can choose a d large enough that that d dominates the constant hidden by the theta, which gives us dn^2 >= d >= T(n) <-- (the constant being dominated).
# Irregular Example
Recurrence: T(n) = T(n/3) + T(2n/3) + theta(n).

Let c be the upper-bound constant hidden by the theta(n) term for n >= n0. There are actually two threshold constants here - one for the threshold in the recurrence, and one for the threshold in the theta-notation, so we'll let n0 be the higher of the two constants.

The height of the tree runs down the right edge of the tree, corresponding to problems of size n, (2/3)n, ..., theta(1) with costs bounded by cn, c(2n/3), ..., theta(1) respectively. We hit the rightmost leaf when $(2/3)^h$n < $n_0$ <= $(2/3)^{h-1}n$. To explain this, the (2/3) must be applied enough times that it's smaller than n0 but the previous application was not smaller than n0. Then the value of h = floor($log_{3/2}(n/n_0)$) + 1. This goofy value comes from us wanting this:
x = $log_{3/2}(n/n_0)$ 
$(2/3)^hn = (2/3)^{floor(x) + 1}n < (2/3)^xn = (n_0/n)n = n_0$
and
$(2/3)^{h-1}n=(2/3)^{floor(x)}n > (2/3)^xn = (n_0/n)n = n_0$

Therefore the height of the tree is h = theta(lgn).