Most general method in Chapter 4.

Two steps:
1. Guess the form of the solution using symbolic constants
2. Use mathematical induction to show that the solution works, and find the constants.
To apply the inductive hypothesis of induction, you substitute the guessed solution for the function on smaller values.

Can be used to establish an upper or lower bound on a recurrence. It's best not to do them both at the same time. Do them one by one. 

# Example
T(n) = 2T(floor(n/2)) + theta(n)

This is similar to the one for merge sort except for the floor. Let's assume that the upper bound is the same as merge sort: nlgn.

##### Inductive Hypothesis
T(n) < cnlgn for all n > n0, where we choose c and n0 later. We can't assume with the inductive hypothesis that T(n) = O(nlgn) because it's be dangerous - we'll talk about it later.

##### Inductive Step
Assume that the above bound holds for all numbers at least as big as n0 and less than n. Therefore, if n >= 2 * n0, it holds for n/2, giving us
![[Pasted image 20240920202045.png]]

If we substitute the right hand side into our recurrence, we get:
![[Pasted image 20240920202150.png]]

where the last step holds if we constrain n0 and c such that for n >= 2 * n0, cn dominates the unknown function represented by theta(n).

###### Base Case
We want to show
![[Pasted image 20240920202414.png]]

As long as n0 > 1, we have lgn > 0, which implies that nlgn > 0. So let's pick n0 = 2. The base cases should then be T(2) and T(3) (2 <= n < 4). Since the recurrence omits its base cases, they should constant (textbook says they should be if they describe the worst-case running time of any real program on inputs of size 2 or 3). Picking c = max{T(2), T(3)} gives us T(2) <= c < (2lg2)c and T(3) <= c < (3lg3)c, giving the inductive hypothesis for the base cases.

Usually people don't do their proofs this thoroughly, especially the base cases. This is because ~~Tony Stark learns from his mistakes~~ divide and conquer recurrences usually handle base cases the same. Ground the induction on a range of values from a convenient positive constant n0 up to some constant n'0 > n0 such that for n >= n'0, the recurrence always bottoms out on a constant sized base case between n0 and n'0 (in this example n'0 = 2n0). Then, it's usually apparent, without spelling out the details, that with a suitably large choice of the leading constant (c in this example) the inductive hypothesis can be made to hold for all values in the range from n0 to n'0

# Making a good guess
Just have to get experience. Using recursion trees can also be helpful (we learn in 4.4).

If a recurrence is similar to one you've seen before, then it's reasonable to guess a similar solution.

For example,
![[Pasted image 20240921164458.png]]
This recurrence looks like the merge-sort recurrence except for + 17. However, the + 17 becomes negligible when n is large. It turns out that this recurrence is also O(nlgn).

Another way to make good guesses is to make loose upper and lower bounds. For example
![[Pasted image 20240921164658.png]]
You might guess this is omega(n) since the recurrence has the term omega(n). 
Then split your time between lowering the upper bound and raising the lower bound until you converge on the correct one.
# Trick: Subtracting a Low-Order Term
Sometimes you might guess a asymptotic bound but won't be able to do the inductive proof. Usually the issue is that the inductive hypothesis is not strong enough, which you can resolve by subtracting a lower order term.

![[Pasted image 20240921164929.png]]
If you guess that it's O(n) and substitute, you get
![[Pasted image 20240921165032.png]]
so we are off by theta(1).

We can try subtracting a constant d >= 0 from cn. If we substitute now we get:
![[Pasted image 20240921165328.png]]
as long as we choose d to be larger than theta(1). Then in the base cases choose the constant c large enough for cn - d to dominate the implicit base cases.
# Avoiding Pitfalls
Avoid using asymptotic notation while using the substitution method. It hides constants which can make you wrong. It's best to name constants explicitly.

# Induction for substitution in my words
Suppose in the IH the bound that you want to prove. Then for IS, suppose IH is true for numbers x where n0 <= x < n. (This is the same as normal assumption for IS and n0 is added because IH doesn't have to be true for numbers below n0)

Also let n >= f(n0) where f(n) is some function applied to n0 to turn that side into what you want. For example, if you want assumption to apply for T(n/2), then n >= 2 * n0, so that assumption holds for T(n/2). For T(n - 1), you want n >= n0 + 1. Then, substitute whatever that is into the inequality
T(n) < whatever
and try to get that 
T(n) <= bound that you want + some other thing
then you can say
bound that you want + some other thing <= bound that you want,
as long as some other thing <= 0. 
Then use that to inform your choices of n0 and c. Make sure to choose n0 and c such that the function exists for all values and > 0 (like if T(n) = O(lgn), choose n0 >= 2).

Now for your base cases, these are the values bigger than n0 but smaller than the modification you made to n0 at the beginning of IS. (like, if you did n >= 2 * n0, then the base cases are those values of n >= n0 but smaller than 2 * n0, aka those in n0 <= n < 2 * n0). Since the algorithm will usually be algorithmic you can assume these are constant and choose a c appropriate for these. It seems standard to say c = max(all constrain values on c). For example, if c >= 1 and you want T(2) <= c2lg2 and T(3) <= c3lg3, then choose c = max(1, T(2)/2, T(3)/(3lg3))
