3.2-1
max{f(n), g(n)} = theta(f(n)+g(n))
Use definition of theta notation to prove that the function max{f(n), g(n)}
is >= c1(f(n) + g(n)) and <= c2(f(n) + g(n)) for some constants c1
and c2

3.2-2
Saying that an algorithm's running time is at least O(n^2) means that
T(n) >= f(n) where f(n) = O(n^2). So, we can choose any function like
f(n) = 0 and then the statement will be true for any T(n).

3.2-3
Yes c = 2 and n0 = 1. No. 2^2n = 2^n * 2^n and we want this to be <= c2^n.
This would imply we want c >= 2^n, which is not possible.

3.2-4
Suppose that we have f(n) = O(g(n)) and f(n) = omega(g(n)). Then there exists
some c1 such that:
0 <= f(n) <= c1g(n)
and some c2 such that:
0 <= c2g(n) <= f(n).

Combining these inequalities, we get:
0 <= c2g(n) <= f(n) <= c1g(n).

thus, if f(n) = O(g(n)) and f(n) = omega(g(n)) then f(n) = theta(g(n)).

Suppose that we have f(n) = theta(g(n)).
Then we'd have that a c1 and c2 exist such that:
0 <= c1g(n) <= f(n) <= c2g(n).

Knowing this, we then know that there exists a c such that
0 <= cg(n) <= f(n), where c is just c1.
Therefore f(n) = omega(g(n))

Knowing this, we then know that there exists a c such that
0 <= f(n) <= cg(n) , where c is just c2.
Therefore f(n) = O(g(n))

3.2-5
If an algorithm's worst-case running time is O(g(n)), then that means
that the algorithm's longest running time is bounded by:
0 <= f(n) <= cg(n), where c is some constant
past n0.

If an algorithm's best case running time is O(g(n)), then that means that the
algorithm's shortest running time is bounded by:
0 <= cg(n) <= f(n), where c is some constant
past n0.

The rest of the algorithm's running times must be somewhere in between the
worst and best case running times. Since both the worst and best case running
times run at g(n), the running time of the alg in all cases is characterized by
the two inequalities:

0 <= f(n) <= c1g(n)
0 <= c2g(n) <= f(n)

Then we can combine them to get f(n) = theta(g(n))

Suppose that f(n) = theta(g(n)).
Then we know that there exist c1, c2, and n0 such that
0 <= c1g(n) <= f(n) <= c2g(n)
for n past n0

Since theta(g(n)) is tight, it describes both the best and worst cases of an algorithm's running time. As a result, we can say that 
0 <= cg(n) <= f(n) in the worst case.
and 
0 <= f(n) <= cg(n) in the best case.

Therefore yada yada yada